{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nmt.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"TjPTaRB4mpCd","colab_type":"text"},"cell_type":"markdown","source":["# Colab FAQ\n","\n","For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n","\n","You need to use the colab GPU for this assignmentby selecting:\n","\n","> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"]},{"metadata":{"id":"s9IS9B9-yUU5","colab_type":"text"},"cell_type":"markdown","source":["## Setup PyTorch\n","All files are stored at /content/csc421/a3/ folder\n"]},{"metadata":{"id":"Z-6MQhMOlHXD","colab_type":"code","outputId":"aab38e2a-5ed9-43a5-e114-149c73ac1675","executionInfo":{"status":"ok","timestamp":1555982619166,"user_tz":240,"elapsed":8097,"user":{"displayName":"Taehoon Jun","photoUrl":"","userId":"07855264162456456560"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"cell_type":"code","source":["######################################################################\n","# Setup python environment and change the current working directory\n","######################################################################\n","!pip install torch torchvision\n","!pip install Pillow==4.0.0\n","%mkdir -p /content/csc421/a3/\n","%cd /content/csc421/a3"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.2)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n","Collecting Pillow==4.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.6MB)\n","\u001b[K    100% |████████████████████████████████| 5.6MB 7.5MB/s \n","\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n","\u001b[31mtorchvision 0.2.2.post3 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n","\u001b[31mscikit-image 0.14.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n","\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n","Installing collected packages: Pillow\n","  Found existing installation: Pillow 4.3.0\n","    Uninstalling Pillow-4.3.0:\n","      Successfully uninstalled Pillow-4.3.0\n","Successfully installed Pillow-4.0.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["/content/csc421/a3\n"],"name":"stdout"}]},{"metadata":{"id":"9DaTdRNuUra7","colab_type":"text"},"cell_type":"markdown","source":["# Helper code"]},{"metadata":{"id":"4BIpGwANoQOg","colab_type":"text"},"cell_type":"markdown","source":["## Utility functions"]},{"metadata":{"id":"D-UJHBYZkh7f","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import pdb\n","import argparse\n","import pickle as pkl\n","\n","from collections import defaultdict\n","\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","from six.moves.urllib.request import urlretrieve\n","import tarfile\n","import pickle\n","import sys\n","\n","\n","def get_file(fname,\n","             origin,\n","             untar=False,\n","             extract=False,\n","             archive_format='auto',\n","             cache_dir='data'):\n","    datadir = os.path.join(cache_dir)\n","    if not os.path.exists(datadir):\n","        os.makedirs(datadir)\n","\n","    if untar:\n","        untar_fpath = os.path.join(datadir, fname)\n","        fpath = untar_fpath + '.tar.gz'\n","    else:\n","        fpath = os.path.join(datadir, fname)\n","    \n","    \n","    print(fpath)\n","    if not os.path.exists(fpath):\n","        print('Downloading data from', origin)\n","\n","        error_msg = 'URL fetch failure on {}: {} -- {}'\n","        try:\n","            try:\n","                urlretrieve(origin, fpath)\n","            except URLError as e:\n","                raise Exception(error_msg.format(origin, e.errno, e.reason))\n","            except HTTPError as e:\n","                raise Exception(error_msg.format(origin, e.code, e.msg))\n","        except (Exception, KeyboardInterrupt) as e:\n","            if os.path.exists(fpath):\n","                os.remove(fpath)\n","            raise\n","\n","    if untar:\n","        if not os.path.exists(untar_fpath):\n","            print('Extracting file.')\n","            with tarfile.open(fpath) as archive:\n","                archive.extractall(datadir)\n","        return untar_fpath\n","\n","    if extract:\n","        _extract_archive(fpath, datadir, archive_format)\n","\n","    return fpath\n","\n","class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","        \n","def to_var(tensor, cuda):\n","    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n","\n","        Arguments:\n","            tensor: A Tensor object.\n","            cuda: A boolean flag indicating whether to use the GPU.\n","\n","        Returns:\n","            A Variable object, on the GPU if cuda==True.\n","    \"\"\"\n","    if cuda:\n","        return Variable(tensor.cuda())\n","    else:\n","        return Variable(tensor)\n","\n","\n","def create_dir_if_not_exists(directory):\n","    \"\"\"Creates a directory if it doesn't already exist.\n","    \"\"\"\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","\n","def save_loss_plot(train_losses, val_losses, opts):\n","    \"\"\"Saves a plot of the training and validation loss curves.\n","    \"\"\"\n","    plt.figure()\n","    plt.plot(range(len(train_losses)), train_losses)\n","    plt.plot(range(len(val_losses)), val_losses)\n","    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n","    plt.xlabel('Epochs', fontsize=16)\n","    plt.ylabel('Loss', fontsize=16)\n","    plt.xticks(fontsize=14)\n","    plt.yticks(fontsize=14)\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n","    plt.close()\n","\n","\n","def checkpoint(encoder, decoder, idx_dict, opts):\n","    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n","    contains the char_to_index and index_to_char mappings, and the start_token\n","    and end_token values.\n","    \"\"\"\n","    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n","        torch.save(encoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n","        torch.save(decoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n","        pkl.dump(idx_dict, f)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pbvpn4MaV0I1","colab_type":"text"},"cell_type":"markdown","source":["## Data loader"]},{"metadata":{"id":"XVT4TNTOV3Eg","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_lines(filename):\n","    \"\"\"Read a file and split it into lines.\n","    \"\"\"\n","    lines = open(filename).read().strip().lower().split('\\n')\n","    return lines\n","\n","\n","def read_pairs(filename):\n","    \"\"\"Reads lines that consist of two words, separated by a space.\n","\n","    Returns:\n","        source_words: A list of the first word in each line of the file.\n","        target_words: A list of the second word in each line of the file.\n","    \"\"\"\n","    lines = read_lines(filename)\n","    source_words, target_words = [], []\n","    for line in lines:\n","        line = line.strip()\n","        if line:\n","            source, target = line.split()\n","            source_words.append(source)\n","            target_words.append(target)\n","    return source_words, target_words\n","\n","\n","def all_alpha_or_dash(s):\n","    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n","    \"\"\"\n","    return all(c.isalpha() or c == '-' for c in s)\n","\n","\n","def filter_lines(lines):\n","    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n","    \"\"\"\n","    return [line for line in lines if all_alpha_or_dash(line)]\n","\n","\n","def load_data():\n","    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n","    \"\"\"\n","\n","    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n","\n","    # Filter lines\n","    source_lines = filter_lines(source_lines)\n","    target_lines = filter_lines(target_lines)\n","\n","    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n","\n","    # Create a dictionary mapping each character to a unique index\n","    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n","\n","    # Add start and end tokens to the dictionary\n","    start_token = len(char_to_index)\n","    end_token = len(char_to_index) + 1\n","    char_to_index['SOS'] = start_token\n","    char_to_index['EOS'] = end_token\n","\n","    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n","    index_to_char = { index: char for (char, index) in char_to_index.items() }\n","\n","    # Store the final size of the vocabulary\n","    vocab_size = len(char_to_index)\n","\n","    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n","\n","    idx_dict = { 'char_to_index': char_to_index,\n","                 'index_to_char': index_to_char,\n","                 'start_token': start_token,\n","                 'end_token': end_token }\n","\n","    return line_pairs, vocab_size, idx_dict\n","\n","\n","def create_dict(pairs):\n","    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n","    This is used to make batches: each batch consists of two parallel tensors, one containing\n","    all source indexes and the other containing all corresponding target indexes.\n","    Within a batch, all the source words are the same length, and all the target words are\n","    the same length.\n","    \"\"\"\n","    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n","\n","    d = defaultdict(list)\n","    for (s,t) in unique_pairs:\n","        d[(len(s), len(t))].append((s,t))\n","\n","    return d\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bRWfRdmVVjUl","colab_type":"text"},"cell_type":"markdown","source":["## Training and evaluation code"]},{"metadata":{"id":"wa5-onJhoSeM","colab_type":"code","colab":{}},"cell_type":"code","source":["def string_to_index_list(s, char_to_index, end_token):\n","    \"\"\"Converts a sentence into a list of indexes (for each character).\n","    \"\"\"\n","    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n","\n","\n","def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n","    words (whitespace-separated), running the encoder-decoder model to translate each\n","    word independently, and then stitching the words back together with spaces between them.\n","    \"\"\"\n","    if idx_dict is None:\n","      line_pairs, vocab_size, idx_dict = load_data()\n","    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n","\n","\n","def translate(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a given string from English to Pig-Latin.\n","    \"\"\"\n","\n","    char_to_index = idx_dict['char_to_index']\n","    index_to_char = idx_dict['index_to_char']\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","\n","    max_generated_chars = 20\n","    gen_string = ''\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_last_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_last_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    for i in range(max_generated_chars):\n","      ## slow decoding, recompute everything at each time\n","      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n","      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","      ni = ni[-1] #latest output token\n","\n","      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","      \n","      if ni == end_token:\n","          break\n","      else:\n","          gen_string = \"\".join(\n","              [index_to_char[int(item)] \n","               for item in generated_words.cpu().numpy().reshape(-1)])\n","\n","    return gen_string\n","\n","\n","def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n","    \"\"\"\n","    if idx_dict is None:\n","      line_pairs, vocab_size, idx_dict = load_data()\n","    char_to_index = idx_dict['char_to_index']\n","    index_to_char = idx_dict['index_to_char']\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","\n","    max_generated_chars = 20\n","    gen_string = ''\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    produced_end_token = False\n","\n","    for i in range(max_generated_chars):\n","      ## slow decoding, recompute everything at each time\n","      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n","      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","      ni = ni[-1] #latest output token\n","      \n","      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","      \n","      if ni == end_token:\n","          break\n","      else:\n","          gen_string = \"\".join(\n","              [index_to_char[int(item)] \n","               for item in generated_words.cpu().numpy().reshape(-1)])\n","    \n","    if isinstance(attention_weights, tuple):\n","      ## transformer's attention mweights\n","      attention_weights, self_attention_weights = attention_weights\n","    \n","    all_attention_weights = attention_weights.data.cpu().numpy()\n","    \n","    for i in range(len(all_attention_weights)):\n","      attention_weights_matrix = all_attention_weights[i].squeeze()\n","      fig = plt.figure()\n","      ax = fig.add_subplot(111)\n","      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n","      fig.colorbar(cax)\n","\n","      # Set up axes\n","      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n","      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n","\n","      # Show label at every tick\n","      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","      # Add title\n","      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n","      plt.tight_layout()\n","      plt.grid('off')\n","      plt.show()\n","      #plt.savefig(save)\n","\n","      #plt.close(fig)\n","\n","    return gen_string\n","\n","\n","def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n","    \"\"\"Train/Evaluate the model on a dataset.\n","\n","    Arguments:\n","        data_dict: The validation/test word pairs, organized by source and target lengths.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n","        opts: The command-line arguments.\n","\n","    Returns:\n","        mean_loss: The average loss over all batches from data_dict.\n","    \"\"\"\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","    char_to_index = idx_dict['char_to_index']\n","\n","    losses = []\n","    for key in data_dict:\n","        input_strings, target_strings = zip(*data_dict[key])\n","        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n","        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n","\n","        num_tensors = len(input_tensors)\n","        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n","\n","        for i in range(num_batches):\n","\n","            start = i * opts.batch_size\n","            end = start + opts.batch_size\n","\n","            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n","            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n","\n","            # The batch size may be different in each epoch\n","            BS = inputs.size(0)\n","\n","            encoder_annotations, encoder_hidden = encoder(inputs)\n","\n","            # The last hidden state of the encoder becomes the first hidden state of the decoder\n","            decoder_hidden = encoder_hidden\n","\n","            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n","            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n","\n","            loss = 0.0\n","\n","            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n","\n","            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n","            \n","            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n","            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n","            targets_flatten = targets.view(-1)\n","            loss = criterion(decoder_outputs_flatten, targets_flatten)\n","\n","            losses.append(loss.item())\n","\n","            ## training if an optimizer is provided\n","            if optimizer:\n","              # Zero gradients\n","              optimizer.zero_grad()\n","              # Compute gradients\n","              loss.backward()\n","              # Update the parameters of the encoder and decoder\n","              optimizer.step()\n","              \n","    mean_loss = np.mean(losses)\n","    return mean_loss\n","\n","  \n","\n","def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n","    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n","        * Prints training and val loss each epoch.\n","        * Prints qualitative translation results each epoch using TEST_SENTENCE\n","        * Saves an attention map for TEST_WORD_ATTN each epoch\n","\n","    Arguments:\n","        train_dict: The training word pairs, organized by source and target lengths.\n","        val_dict: The validation word pairs, organized by source and target lengths.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n","        opts: The command-line arguments.\n","    \"\"\"\n","\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","    char_to_index = idx_dict['char_to_index']\n","\n","    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n","\n","    best_val_loss = 1e6\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(opts.nepochs):\n","\n","        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n","        \n","        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n","        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n","\n","        if val_loss < best_val_loss:\n","            checkpoint(encoder, decoder, idx_dict, opts)\n","\n","        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n","        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n","\n","        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n","        loss_log.flush()\n","\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","\n","        save_loss_plot(train_losses, val_losses, opts)\n","\n","\n","def print_data_stats(line_pairs, vocab_size, idx_dict):\n","    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n","    \"\"\"\n","    print('=' * 80)\n","    print('Data Stats'.center(80))\n","    print('-' * 80)\n","    for pair in line_pairs[:5]:\n","        print(pair)\n","    print('Num unique word pairs: {}'.format(len(line_pairs)))\n","    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n","    print('Vocab size: {}'.format(vocab_size))\n","    print('=' * 80)\n","\n","\n","def train(opts):\n","    line_pairs, vocab_size, idx_dict = load_data()\n","    print_data_stats(line_pairs, vocab_size, idx_dict)\n","\n","    # Split the line pairs into an 80% train and 20% val split\n","    num_lines = len(line_pairs)\n","    num_train = int(0.8 * num_lines)\n","    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n","\n","    # Group the data by the lengths of the source and target words, to form batches\n","    train_dict = create_dict(train_pairs)\n","    val_dict = create_dict(val_pairs)\n","\n","    ##########################################################################\n","    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n","    ##########################################################################\n","    encoder = GRUEncoder(vocab_size=vocab_size, \n","                         hidden_size=opts.hidden_size, \n","                         opts=opts)\n","\n","    if opts.decoder_type == 'rnn':\n","        decoder = RNNDecoder(vocab_size=vocab_size, \n","                             hidden_size=opts.hidden_size)\n","    elif opts.decoder_type == 'rnn_attention':\n","        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n","                                      hidden_size=opts.hidden_size, \n","                                      attention_type=opts.attention_type)\n","    elif opts.decoder_type == 'transformer':\n","        decoder = TransformerDecoder(vocab_size=vocab_size, \n","                                     hidden_size=opts.hidden_size, \n","                                     num_layers=opts.num_transformer_layers)\n","    else:\n","        raise NotImplementedError\n","        \n","    #### setup checkpoint path\n","    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n","                                      opts.batch_size, \n","                                      opts.decoder_type)\n","    opts.checkpoint_path = model_name\n","    create_dir_if_not_exists(opts.checkpoint_path)\n","    ####\n","\n","    if opts.cuda:\n","        encoder.cuda()\n","        decoder.cuda()\n","        print(\"Moved models to GPU!\")\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n","\n","    try:\n","        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n","    except KeyboardInterrupt:\n","        print('Exiting early from training.')\n","        return encoder, decoder\n","      \n","    return encoder, decoder\n","\n","\n","def print_opts(opts):\n","    \"\"\"Prints the values of all command-line arguments.\n","    \"\"\"\n","    print('=' * 80)\n","    print('Opts'.center(80))\n","    print('-' * 80)\n","    for key in opts.__dict__:\n","        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n","    print('=' * 80)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bXNsLNkOn38w","colab_type":"text"},"cell_type":"markdown","source":["# Your code for NMT models"]},{"metadata":{"id":"_BAfi_8yWB3y","colab_type":"text"},"cell_type":"markdown","source":["## GRU cell"]},{"metadata":{"id":"9ztmyA5Ro67o","colab_type":"code","colab":{}},"cell_type":"code","source":["class MyGRUCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(MyGRUCell, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        ## Input linear layers\n","        self.Wiz = nn.Linear(input_size, hidden_size)\n","        self.Wir = nn.Linear(input_size, hidden_size)\n","        self.Wih = nn.Linear(input_size, hidden_size)\n","\n","        ## Hidden linear layers\n","        self.Whz = nn.Linear(hidden_size, hidden_size)\n","        self.Whr = nn.Linear(hidden_size, hidden_size)\n","        self.Whh = nn.Linear(hidden_size, hidden_size)\n","        \n","\n","\n","    def forward(self, x, h_prev):\n","        \"\"\"Forward pass of the GRU computation for one time step.\n","\n","        Arguments\n","            x: batch_size x input_size\n","            h_prev: batch_size x hidden_size\n","\n","        Returns:\n","            h_new: batch_size x hidden_size\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        z = F.sigmoid(self.Wiz.forward(x) + self.Whr.forward(h_prev))\n","        r = F.sigmoid(self.Wir.forward(x) + self.Whr.forward(h_prev))\n","        g = F.tanh(self.Wih.forward(x) + r*(self.Whh.forward(h_prev)))\n","     \n","        h_new = (1-z)*g + z*h_prev\n","        return h_new\n","\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"-JBVFLEZWNC1","colab_type":"text"},"cell_type":"markdown","source":["### GRU encoder / decoder"]},{"metadata":{"id":"xaDt7XDmWRzC","colab_type":"code","colab":{}},"cell_type":"code","source":["class GRUEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, opts):\n","        super(GRUEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.gru = nn.GRUCell(hidden_size, hidden_size)\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        hidden = self.init_hidden(batch_size)\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","        annotations = []\n","\n","        for i in range(seq_len):\n","            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n","            hidden = self.gru(x, hidden)\n","            annotations.append(hidden)\n","\n","        annotations = torch.stack(annotations, dim=1)\n","        return annotations, hidden\n","\n","    def init_hidden(self, bs):\n","        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n","        of a batch of sequences.\n","\n","        Arguments:\n","            bs: The batch size for the initial hidden state.\n","\n","        Returns:\n","            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n","        \"\"\"\n","        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)\n","\n","\n","class RNNDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super(RNNDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the non-attentional decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch. (batch_size x seq_len)\n","            annotations: This is not used here. It just maintains consistency with the\n","                    interface used by the AttentionDecoder class.\n","            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            None\n","        \"\"\"        \n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n","\n","        hiddens = []\n","        h_prev = hidden_init\n","        for i in range(seq_len):\n","            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n","            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n","            hiddens.append(h_prev)\n","\n","        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n","        \n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, None      \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tWe0RO5FWajD","colab_type":"text"},"cell_type":"markdown","source":["## Attention"]},{"metadata":{"id":"9GUK5A7CWhV8","colab_type":"code","colab":{}},"cell_type":"code","source":["class AdditiveAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(AdditiveAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        # A two layer fully-connected network\n","        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n","        self.attention_network = nn.Sequential(\n","                                    nn.Linear(hidden_size*2, hidden_size),\n","                                    nn.ReLU(),\n","                                    nn.Linear(hidden_size, 1)\n","                                 )\n","\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the additive attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state. (batch_size x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x 1 x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The attention_weights must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size = queries.size()[0]\n","        expanded_queries = torch.unsqueeze(queries, 1).expand_as(keys)\n","        concat_inputs = torch.cat((expanded_queries, keys), 2)\n","        unnormalized_attention = self.attention_network(concat_inputs)\n","        attention_weights = self.softmax(unnormalized_attention)\n","        context = torch.sum(attention_weights*values, dim=1)\n","        return context, attention_weights\n","        \n","      \n","\n","class ScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(ScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size = queries.size()[0]\n","        qur = queries\n","        if queries.dim() < 3:\n","          qur = torch.unsqueeze(queries, 1)\n","        q = self.Q.forward(qur)\n","        k = self.K.forward(keys)\n","        v = self.V.forward(values)\n","        unnormalized_attention = (torch.bmm(k, q.permute(0,2,1)))*self.scaling_factor\n","        attention_weights = self.softmax(unnormalized_attention)\n","        context = torch.sum(torch.bmm(attention_weights.permute(0,2,1), v), dim=1)\n","        return context, attention_weights\n","        \n","\n","      \n","      \n","class CausalScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(CausalScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.neg_inf = torch.tensor(-1e7)\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size = queries.size()[0]\n","        qur = queries\n","        if queries.dim() < 3:\n","          qur = torch.unsqueeze(queries, 1)\n","        q = self.Q.forward(qur)\n","        k = self.K.forward(keys)\n","        v = self.V.forward(values)\n","        unnormalized_attention = (torch.bmm(k, q.permute(0,2,1)))*self.scaling_factor\n","        mask = unnormalized_attention\n","        #print(\"unnormalized_attetion: \", mask.size())\n","        for i in range(unnormalized_attention.size()[1]): #seq_len\n","          mask[:,i,:] = torch.tril(mask[:,i,:])\n","        attention_weights = self.softmax(mask)\n","        #print(\"attention_weights size: \", attention_weights.size())\n","        #print(\"value size: \", v.size())\n","        context = torch.sum(torch.bmm(attention_weights.permute(0,2,1), v), dim=1)\n","        return context, attention_weights\n","        \n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"pemjZo2XWtRt","colab_type":"text"},"cell_type":"markdown","source":["### Attention decoder"]},{"metadata":{"id":"PfjF0Z-PWwPv","colab_type":"code","colab":{}},"cell_type":"code","source":["class RNNAttentionDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n","        super(RNNAttentionDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n","        if attention_type == 'additive':\n","          self.attention = AdditiveAttention(hidden_size=hidden_size)\n","        elif attention_type == 'scaled_dot':\n","          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n","        \n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","        \n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","        \n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n","\n","        hiddens = []\n","        attentions = []\n","        h_prev = hidden_init\n","        for i in range(seq_len):\n","            # ------------\n","            # FILL THIS IN\n","            # ------------\n","            embed_current = embed[:,i, :]\n","            context, attention_weights = self.attention.forward(h_prev, annotations, annotations)\n","            embed_and_context = torch.cat((embed_current, context), 1)\n","            h_prev = self.rnn.forward(embed_and_context, h_prev)\n","\n","            \n","            hiddens.append(h_prev)\n","            attentions.append(attention_weights)\n","\n","        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n","        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n","        \n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, attentions\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"N8JpcwTRW5cw","colab_type":"text"},"cell_type":"markdown","source":["### Transformer decoder"]},{"metadata":{"id":"V5vJPku1W7sz","colab_type":"code","colab":{}},"cell_type":"code","source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers):\n","        super(TransformerDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n","        self.num_layers = num_layers\n","        \n","        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n","                                    hidden_size=hidden_size, \n","                                 ) for i in range(self.num_layers)])\n","        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n","                                    hidden_size=hidden_size, \n","                                 ) for i in range(self.num_layers)])\n","        self.attention_mlps = nn.ModuleList([nn.Sequential(\n","                                    nn.Linear(hidden_size, hidden_size),\n","                                    nn.ReLU(),\n","                                 ) for i in range(self.num_layers)])\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","        \n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: Not used in the transformer decoder\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","        \n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n","\n","        encoder_attention_weights_list = []\n","        self_attention_weights_list = []\n","        contexts = embed\n","        for i in range(self.num_layers):\n","          # ------------\n","          # FILL THIS IN\n","          # ------------\n","          embed_current = contexts[:,i,:]\n","          #embed_accum = embed_current\n","          #for j in range(i):\n","          #  embed_accum = torch.cat((embed_accum, contexts[:,j,:].unsqueeze(1)), 1)\n","          new_contexts, self_attention_weights = self.self_attentions[i].forward(contexts, annotations, annotations)\n","          \n","          residual_contexts = torch.cat((embed_current, new_contexts), 1) \n","          \n","          new_contexts, encoder_attention_weights = self.encoder_attentions[i].forward(residual_contexts, annotations, annotations)\n","          residual_contexts = torch.cat((residual_contexts, new_contexts), 1)\n","          new_contexts = self.attention_mlps[i].forward(residual_contexts)\n","\n","          contexts = torch.cat((new_contexts, residual_contexts), 1)\n","\n","          \n","          encoder_attention_weights_list.append(encoder_attention_weights)\n","          self_attention_weights_list.append(self_attention_weights)\n","          \n","        output = self.out(contexts)\n","        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n","        self_attention_weights = torch.stack(self_attention_weights_list)\n","        \n","        return output, (encoder_attention_weights, self_attention_weights)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XuNFd6LNo0-o","colab_type":"text"},"cell_type":"markdown","source":["# Training\n"]},{"metadata":{"id":"kiUwiOITHTW4","colab_type":"text"},"cell_type":"markdown","source":["## Download dataset"]},{"metadata":{"id":"xwcFjsEpHRbI","colab_type":"code","outputId":"20ec2c82-ba3a-4d4a-ef6b-6b8e6f9e538c","executionInfo":{"status":"ok","timestamp":1555993143692,"user_tz":240,"elapsed":1076,"user":{"displayName":"Taehoon Jun","photoUrl":"","userId":"07855264162456456560"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":["######################################################################\n","# Download Translation datasets\n","######################################################################\n","data_fpath = get_file(fname='pig_latin_data.txt', \n","                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n","                         untar=False)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["data/pig_latin_data.txt\n","Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_data.txt\n"],"name":"stdout"}]},{"metadata":{"id":"hmQmyJDSRFKR","colab_type":"text"},"cell_type":"markdown","source":["## RNN decoder"]},{"metadata":{"id":"0LKaRF1jwhH7","colab_type":"code","outputId":"9bf83727-10ef-4a29-b3ff-f4e3c33d1219","executionInfo":{"status":"ok","timestamp":1553469934149,"user_tz":240,"elapsed":221193,"user":{"displayName":"Taehoon Jun","photoUrl":"","userId":"07855264162456456560"}},"colab":{"base_uri":"https://localhost:8080/","height":2221}},"cell_type":"code","source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n","              'attention_type': '',  # options: additive / scaled_dot\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","rnn_encoder, rnn_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: rnn                                    \n","                               lr_decay: 0.99                                   \n","                         attention_type:                                        \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNNDecoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:   0 | Train loss: 2.374 | Val loss: 1.988 | Gen: eray ay enssay ensay enssay\n","Epoch:   1 | Train loss: 1.920 | Val loss: 1.846 | Gen: areray ay ongsay ay ongstay\n","Epoch:   2 | Train loss: 1.779 | Val loss: 1.759 | Gen: areray aray ongedway ingedway ountedway\n","Epoch:   3 | Train loss: 1.679 | Val loss: 1.694 | Gen: earteray aray oongedway ingsay ountedway\n","Epoch:   4 | Train loss: 1.596 | Val loss: 1.632 | Gen: eateray ayeday oongedway ingsay ongertay\n","Epoch:   5 | Train loss: 1.528 | Val loss: 1.612 | Gen: earedway aray-ingedway oingedway ingsay ontentedway\n","Epoch:   6 | Train loss: 1.484 | Val loss: 1.586 | Gen: earedway aateday oingenttay ingsay ontentedway\n","Epoch:   7 | Train loss: 1.441 | Val loss: 1.545 | Gen: earedway aray-ingsray oinglay-ortedway ingsay otentedway\n","Epoch:   8 | Train loss: 1.409 | Val loss: 1.529 | Gen: ersay areationsay olingedway istay otentedway\n","Epoch:   9 | Train loss: 1.377 | Val loss: 1.525 | Gen: ersay areationsay olingedway istay otingedway\n","Epoch:  10 | Train loss: 1.345 | Val loss: 1.498 | Gen: ersay areray ollenglay istay orlestay\n","Epoch:  11 | Train loss: 1.331 | Val loss: 1.502 | Gen: ersay areray ollellyway istay ointentedway\n","Epoch:  12 | Train loss: 1.309 | Val loss: 1.459 | Gen: ersay areray ollway-ingsray istay-ingsray ointedsay\n","Epoch:  13 | Train loss: 1.275 | Val loss: 1.459 | Gen: estay-ingsray areray ollway-ingsray isssay ointedway\n","Epoch:  14 | Train loss: 1.257 | Val loss: 1.426 | Gen: ersay areray ollway-ingsray isssay orkentway\n","Epoch:  15 | Train loss: 1.238 | Val loss: 1.425 | Gen: ersay areray ollway-ingsay isssay orkedway\n","Epoch:  16 | Train loss: 1.211 | Val loss: 1.397 | Gen: eray-inway-awlay areray-ingsray oninglay isssay orkedway\n","Epoch:  17 | Train loss: 1.199 | Val loss: 1.428 | Gen: erstay areray-oringway olingedway isssay orkedway\n","Epoch:  18 | Train loss: 1.185 | Val loss: 1.383 | Gen: ersay areray-oringway oninglay isssay oyedway\n","Epoch:  19 | Train loss: 1.164 | Val loss: 1.377 | Gen: ersay areray-outedway oninglay issay oyelay-outedway\n","Epoch:  20 | Train loss: 1.147 | Val loss: 1.381 | Gen: eray-inway-awlay areray-orsingway oninglay-outedway issway ountedway\n","Epoch:  21 | Train loss: 1.140 | Val loss: 1.359 | Gen: eray-aseplay areray onlingsay issay orkentway\n","Epoch:  22 | Train loss: 1.118 | Val loss: 1.350 | Gen: eray-orsingway areray-orsingway onilingway issay orkedway\n","Epoch:  23 | Train loss: 1.118 | Val loss: 1.349 | Gen: ersay areray onilingway issay orkentway\n","Epoch:  24 | Train loss: 1.098 | Val loss: 1.333 | Gen: eray-orsingway areray-outedway onlingsay issay orkedway\n","Epoch:  25 | Train loss: 1.091 | Val loss: 1.319 | Gen: eray airway onillway-awlay issay orkedway\n","Epoch:  26 | Train loss: 1.085 | Val loss: 1.331 | Gen: ersay airway oninglay issay orkentcay\n","Epoch:  27 | Train loss: 1.063 | Val loss: 1.309 | Gen: eray areray-onstray onnilysay issay orkedway\n","Epoch:  28 | Train loss: 1.047 | Val loss: 1.306 | Gen: eray-atesway airway oncilysay issay ourkedway\n","Epoch:  29 | Train loss: 1.038 | Val loss: 1.313 | Gen: eray areray oncillway issay orkedway\n","Epoch:  30 | Train loss: 1.050 | Val loss: 1.303 | Gen: eray-atuedshay airway oncilgslay issay ourkonway\n","Epoch:  31 | Train loss: 1.030 | Val loss: 1.279 | Gen: eray airway oncillway issay orkingshay\n","Epoch:  32 | Train loss: 1.012 | Val loss: 1.294 | Gen: erway airway oncillway issay orkedway\n","Epoch:  33 | Train loss: 1.001 | Val loss: 1.270 | Gen: eray airway oncillway issay ourklyway\n","Epoch:  34 | Train loss: 1.001 | Val loss: 1.261 | Gen: eray airway onconintfay issay ourklay\n","Epoch:  35 | Train loss: 0.998 | Val loss: 1.270 | Gen: eray airway oncingedway issay orkingshay\n","Epoch:  36 | Train loss: 0.989 | Val loss: 1.261 | Gen: eray airway oncilyway issay ourklyway\n","Epoch:  37 | Train loss: 0.984 | Val loss: 1.252 | Gen: eray airway oncilysay issay orkingshay\n","Epoch:  38 | Train loss: 0.976 | Val loss: 1.265 | Gen: eray airway oncingslay issay ourklyway\n","Epoch:  39 | Train loss: 0.973 | Val loss: 1.243 | Gen: eray airway oncingslay issay orkingway\n","Epoch:  40 | Train loss: 0.955 | Val loss: 1.227 | Gen: earday airway oncingslay issay ourtinglay\n","Epoch:  41 | Train loss: 0.980 | Val loss: 1.260 | Gen: eray-aturednay airway oncingslay issay orkingway\n","Epoch:  42 | Train loss: 0.964 | Val loss: 1.221 | Gen: eray airway oncingsray issay ourklyway\n","Epoch:  43 | Train loss: 0.940 | Val loss: 1.206 | Gen: eway airway oncingstay issay ourklyway\n","Epoch:  44 | Train loss: 0.934 | Val loss: 1.220 | Gen: eway airway oncingedway issay ourtinglay\n","Epoch:  45 | Train loss: 0.928 | Val loss: 1.205 | Gen: eway airway oncingshay issay orkingway\n","Epoch:  46 | Train loss: 0.920 | Val loss: 1.235 | Gen: eray airway oncillway issay ourklyway\n","Epoch:  47 | Train loss: 0.922 | Val loss: 1.195 | Gen: earday airway oncilgslay issay orkingway\n","Epoch:  48 | Train loss: 0.901 | Val loss: 1.215 | Gen: eway airway oncillway issay ourtinglay\n","Epoch:  49 | Train loss: 0.913 | Val loss: 1.192 | Gen: earday airway oncingslay issay orkingway\n","Epoch:  50 | Train loss: 0.904 | Val loss: 1.202 | Gen: eway airway oncillway issay orkingway\n","Epoch:  51 | Train loss: 0.905 | Val loss: 1.177 | Gen: earday airway oncilgsray issay orkingway\n","Epoch:  52 | Train loss: 0.889 | Val loss: 1.195 | Gen: eway airway oncillway issay orkingway\n","Epoch:  53 | Train loss: 0.879 | Val loss: 1.194 | Gen: eardray airway oncillway issay ourklyway\n","Epoch:  54 | Train loss: 0.898 | Val loss: 1.181 | Gen: eway airway oncilgslay issay orkingway\n","Epoch:  55 | Train loss: 0.870 | Val loss: 1.163 | Gen: eardlay airway oncilgstay issay ourlysay\n","Epoch:  56 | Train loss: 0.866 | Val loss: 1.177 | Gen: eway airway onconingsray issay orkingway\n","Epoch:  57 | Train loss: 0.869 | Val loss: 1.173 | Gen: eway airway oncillway issay orkingway\n","Epoch:  58 | Train loss: 0.868 | Val loss: 1.166 | Gen: eway-aturednay airway oncillway issay orkingway\n","Epoch:  59 | Train loss: 0.872 | Val loss: 1.165 | Gen: eway airway oncilgentway issay orkingway\n","Epoch:  60 | Train loss: 0.860 | Val loss: 1.162 | Gen: ehay airway oncillway issay orkingway\n","Epoch:  61 | Train loss: 0.866 | Val loss: 1.165 | Gen: eway airway oncilgedway issay orkingway\n","Epoch:  62 | Train loss: 0.849 | Val loss: 1.157 | Gen: eray airway oncillway-ybay issay orkingway\n","Epoch:  63 | Train loss: 0.844 | Val loss: 1.155 | Gen: eray airway oncillway issay orkingway\n","Epoch:  64 | Train loss: 0.833 | Val loss: 1.176 | Gen: eway-aturednay airway oncillyway issay ourlay-inway\n","Epoch:  65 | Train loss: 0.837 | Val loss: 1.155 | Gen: eway-aturednay airway oncillway-awlay issay orkingway\n","Epoch:  66 | Train loss: 0.831 | Val loss: 1.152 | Gen: eray airway oncilgstay issay ourlay-inway-awlay\n","Epoch:  67 | Train loss: 0.828 | Val loss: 1.169 | Gen: eway-aturednay airway oncillway issay ourlay-inway-ayday\n","Epoch:  68 | Train loss: 0.853 | Val loss: 1.151 | Gen: eray airway oncillway issay ourlay-inway-awlay\n","Epoch:  69 | Train loss: 0.822 | Val loss: 1.149 | Gen: eway airway oncillyway issay ourlay-inway\n","Epoch:  70 | Train loss: 0.821 | Val loss: 1.152 | Gen: ehay airway oncillway issay orycantedway\n","Epoch:  71 | Train loss: 0.814 | Val loss: 1.147 | Gen: eway-aturednay airway oncillyway issay ourlay-inway-awlay\n","Epoch:  72 | Train loss: 0.816 | Val loss: 1.165 | Gen: ereday airway oncillway-awlay issay oryingslay\n","Epoch:  73 | Train loss: 0.817 | Val loss: 1.143 | Gen: eray airway oncilgnay issay orkingway\n","Epoch:  74 | Train loss: 0.803 | Val loss: 1.150 | Gen: eray airway oncilgentway issay orkingway\n","Epoch:  75 | Train loss: 0.798 | Val loss: 1.152 | Gen: eray airway oncilgstay issay ouraltlyway\n","Epoch:  76 | Train loss: 0.806 | Val loss: 1.140 | Gen: eardlay airway onciltfay issay orycay-inway\n","Epoch:  77 | Train loss: 0.810 | Val loss: 1.147 | Gen: eray airway oncilgnay issay oryingfay\n","Epoch:  78 | Train loss: 0.792 | Val loss: 1.134 | Gen: ehay airway oncilgstay issay ouraltlyway\n","Epoch:  79 | Train loss: 0.795 | Val loss: 1.149 | Gen: eray airway onciondedsay issay oryingfay\n","Epoch:  80 | Train loss: 0.794 | Val loss: 1.137 | Gen: eray airway oncilgstay issay oryingmay\n","Epoch:  81 | Train loss: 0.781 | Val loss: 1.143 | Gen: eray airway oncilgshay issay oryingmay-ousedway\n","Epoch:  82 | Train loss: 0.776 | Val loss: 1.142 | Gen: eray airway oncilgstay issay oryingfay\n","Epoch:  83 | Train loss: 0.776 | Val loss: 1.131 | Gen: eray airway onciltfay issay oryingfay\n","Epoch:  84 | Train loss: 0.785 | Val loss: 1.148 | Gen: eray airway oncilgentway issay oryingsay\n","Epoch:  85 | Train loss: 0.779 | Val loss: 1.127 | Gen: eray airway oncilgstay issay oryingfay\n","Epoch:  86 | Train loss: 0.767 | Val loss: 1.130 | Gen: eray airway oncilgstay issay oryingfay\n","Epoch:  87 | Train loss: 0.770 | Val loss: 1.131 | Gen: eray-onderay airway oncilgstay issay oryingsay\n","Epoch:  88 | Train loss: 0.760 | Val loss: 1.126 | Gen: eray-onderay airway onciltfay issay oryingmationway\n","Epoch:  89 | Train loss: 0.757 | Val loss: 1.135 | Gen: eray-onderay airway onciltfay issay oryingfay\n","Epoch:  90 | Train loss: 0.767 | Val loss: 1.125 | Gen: eray-ondearay airway oncilgshay issay oryingonsway\n","Epoch:  91 | Train loss: 0.766 | Val loss: 1.138 | Gen: ehay airway oncioncescay issay oryingfay\n","Epoch:  92 | Train loss: 0.764 | Val loss: 1.132 | Gen: eray airway oncilgstay issay oryingfay\n","Epoch:  93 | Train loss: 0.761 | Val loss: 1.123 | Gen: eray-inway airway oncingtingway issay oryingonsray\n","Epoch:  94 | Train loss: 0.759 | Val loss: 1.144 | Gen: eray airway oncilgnay issay oryingfay\n","Epoch:  95 | Train loss: 0.760 | Val loss: 1.124 | Gen: eray-onderay airway oncilgstay issay oryingmay-ousedway\n","Epoch:  96 | Train loss: 0.740 | Val loss: 1.129 | Gen: eray-inway-awlay airway oncingtray issay oryingmay-ousedway\n","Epoch:  97 | Train loss: 0.735 | Val loss: 1.128 | Gen: eray-ondeay-aturedna airway oncilgshay issay oryingmationway\n","Epoch:  98 | Train loss: 0.747 | Val loss: 1.114 | Gen: eray-onderay airway oncingtray issay oryingmationway\n","Epoch:  99 | Train loss: 0.733 | Val loss: 1.121 | Gen: eray-inway airway oncingstay issay oryingmay-ousedway\n","source:\t\tthe air conditioning is working \n","translated:\teray-inway airway oncingstay issay oryingmay-ousedway\n"],"name":"stdout"}]},{"metadata":{"id":"p2kPGj5DFv7a","colab_type":"code","outputId":"13a8b44f-18f3-4f26-c194-28c82190bf66","executionInfo":{"status":"error","timestamp":1555979270011,"user_tz":240,"elapsed":375346,"user":{"displayName":"Taehoon Jun","photoUrl":"","userId":"07855264162456456560"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["TEST_SENTENCE = 'i went shopping'\n","translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-15-6b1f00618749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTEST_SENTENCE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'i went shopping'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"source:\\t\\t{} \\ntranslated:\\t{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'rnn_encoder' is not defined"]}]},{"metadata":{"id":"7cP7nl5NRJbu","colab_type":"text"},"cell_type":"markdown","source":["## RNN attention decoder"]},{"metadata":{"id":"nKlyfbuPDXDR","colab_type":"code","outputId":"9cb6227f-653a-4d51-eade-84dd77e490ce","executionInfo":{"status":"ok","timestamp":1555974998317,"user_tz":240,"elapsed":477469,"user":{"displayName":"Taehoon Jun","photoUrl":"","userId":"07855264162456456560"}},"colab":{"base_uri":"https://localhost:8080/","height":2526}},"cell_type":"code","source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n","              'attention_type': 'additive',  # options: additive / scaled_dot\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","rnn_attn_encoder, rnn_attn_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: rnn_attention                          \n","                               lr_decay: 0.99                                   \n","                         attention_type: additive                               \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","/usr/local/lib/python2.7/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNNAttentionDecoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MyGRUCell. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type AdditiveAttention. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:   0 | Train loss: 2.387 | Val loss: 1.972 | Gen: eereray ay ongeray ay ongay\n","Epoch:   1 | Train loss: 1.856 | Val loss: 1.780 | Gen: eray ay ongentay ay ongray\n","Epoch:   2 | Train loss: 1.636 | Val loss: 1.577 | Gen: ithtay aaray ongntay ay oungnay\n","Epoch:   3 | Train loss: 1.435 | Val loss: 1.372 | Gen: eathtay aariday ongingway insay orgnay\n","Epoch:   4 | Train loss: 1.249 | Val loss: 1.230 | Gen: eway ariray ongingingway isay orsingway\n","Epoch:   5 | Train loss: 1.081 | Val loss: 1.032 | Gen: ethay airay onditiongay isway orsingway\n","Epoch:   6 | Train loss: 0.904 | Val loss: 0.916 | Gen: ethay airay onditiongingway issay orsingway\n","Epoch:   7 | Train loss: 0.838 | Val loss: 0.910 | Gen: ethay airiray onditionway issay orkingway\n","Epoch:   8 | Train loss: 0.742 | Val loss: 0.759 | Gen: ethtay airtay onditiongay issay orkingway\n","Epoch:   9 | Train loss: 0.631 | Val loss: 0.611 | Gen: enthay airay ondimincingway isay orkingway\n","Epoch:  10 | Train loss: 0.558 | Val loss: 0.556 | Gen: eathtay airay onditiongay isay orkingway\n","Epoch:  11 | Train loss: 0.488 | Val loss: 0.558 | Gen: eaththay airay onditionway isay orkingway\n","Epoch:  12 | Train loss: 0.443 | Val loss: 0.557 | Gen: epthay airay onditionway issay orkingway\n","Epoch:  13 | Train loss: 0.399 | Val loss: 0.540 | Gen: ewhay arisay onditionway isay orkingway\n","Epoch:  14 | Train loss: 0.367 | Val loss: 0.464 | Gen: epthay airay onditiongay isaway orkingway\n","Epoch:  15 | Train loss: 0.338 | Val loss: 0.448 | Gen: ewhay airway onditiongcay isway orkingway\n","Epoch:  16 | Train loss: 0.326 | Val loss: 0.426 | Gen: eptay airway onditioningcay isay orkingway\n","Epoch:  17 | Train loss: 0.259 | Val loss: 0.419 | Gen: eptay arisay onditioningcay isway orkingway\n","Epoch:  18 | Train loss: 0.225 | Val loss: 0.346 | Gen: eptay airway onditionway isway orkingway\n","Epoch:  19 | Train loss: 0.208 | Val loss: 0.338 | Gen: ewhay airway onditioningcay isway orkingway\n","Epoch:  20 | Train loss: 0.192 | Val loss: 0.314 | Gen: ethay airway onditionway isway orkingway\n","Epoch:  21 | Train loss: 0.132 | Val loss: 0.268 | Gen: ethay airway onditioningcay isay orkingway\n","Epoch:  22 | Train loss: 0.132 | Val loss: 0.311 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  23 | Train loss: 0.124 | Val loss: 0.233 | Gen: ehtay airway onditioningcay isway orkingway\n","Epoch:  24 | Train loss: 0.111 | Val loss: 0.262 | Gen: ethay airway onditionway isway orkingway\n","Epoch:  25 | Train loss: 0.118 | Val loss: 0.280 | Gen: ehtay airway onditionwningcay isway orkingway\n","Epoch:  26 | Train loss: 0.180 | Val loss: 0.227 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  27 | Train loss: 0.114 | Val loss: 0.261 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  28 | Train loss: 0.116 | Val loss: 0.176 | Gen: ethay airway onditioningcay isay orkingway\n","Epoch:  29 | Train loss: 0.081 | Val loss: 0.184 | Gen: edtay airway onditioningcay isway orkingway\n","Epoch:  30 | Train loss: 0.154 | Val loss: 0.384 | Gen: ehthay airway onditionway isway orkingway\n","Epoch:  31 | Train loss: 0.279 | Val loss: 0.238 | Gen: ehay airway onditionway isway orkingway\n","Epoch:  32 | Train loss: 0.112 | Val loss: 0.226 | Gen: ethay airway onditionway isay orkingway\n","Epoch:  33 | Train loss: 0.093 | Val loss: 0.235 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  34 | Train loss: 0.092 | Val loss: 0.164 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  35 | Train loss: 0.058 | Val loss: 0.134 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  36 | Train loss: 0.041 | Val loss: 0.153 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  37 | Train loss: 0.037 | Val loss: 0.118 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  38 | Train loss: 0.041 | Val loss: 0.151 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  39 | Train loss: 0.043 | Val loss: 0.108 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  40 | Train loss: 0.028 | Val loss: 0.119 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  41 | Train loss: 0.027 | Val loss: 0.112 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  42 | Train loss: 0.076 | Val loss: 0.237 | Gen: edthay airway onditionway isway orkingway\n","Epoch:  43 | Train loss: 0.243 | Val loss: 0.711 | Gen: ehthay airway onditioggcay isway orkingway\n","Epoch:  44 | Train loss: 0.191 | Val loss: 0.244 | Gen: ehthay airway onditionway isway orkingway\n","Epoch:  45 | Train loss: 0.080 | Val loss: 0.159 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  46 | Train loss: 0.093 | Val loss: 0.181 | Gen: edthay airway onditioningcay isway orkingway\n","Epoch:  47 | Train loss: 0.147 | Val loss: 0.189 | Gen: edhay airway onditioningcay isway orkingway\n","Epoch:  48 | Train loss: 0.068 | Val loss: 0.136 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  49 | Train loss: 0.029 | Val loss: 0.113 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  50 | Train loss: 0.020 | Val loss: 0.105 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  51 | Train loss: 0.016 | Val loss: 0.101 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  52 | Train loss: 0.014 | Val loss: 0.100 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  53 | Train loss: 0.012 | Val loss: 0.099 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  54 | Train loss: 0.011 | Val loss: 0.096 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  55 | Train loss: 0.010 | Val loss: 0.096 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  56 | Train loss: 0.010 | Val loss: 0.093 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  57 | Train loss: 0.009 | Val loss: 0.094 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  58 | Train loss: 0.008 | Val loss: 0.093 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  59 | Train loss: 0.007 | Val loss: 0.089 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  60 | Train loss: 0.007 | Val loss: 0.091 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  61 | Train loss: 0.006 | Val loss: 0.090 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  62 | Train loss: 0.006 | Val loss: 0.089 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  63 | Train loss: 0.005 | Val loss: 0.088 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  64 | Train loss: 0.005 | Val loss: 0.095 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  65 | Train loss: 0.005 | Val loss: 0.089 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  66 | Train loss: 0.005 | Val loss: 0.089 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  67 | Train loss: 0.007 | Val loss: 0.089 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  68 | Train loss: 0.134 | Val loss: 0.460 | Gen: eththththay airway onditioncay isway orkingway\n","Epoch:  69 | Train loss: 0.157 | Val loss: 0.179 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  70 | Train loss: 0.057 | Val loss: 0.116 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  71 | Train loss: 0.019 | Val loss: 0.108 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  72 | Train loss: 0.011 | Val loss: 0.085 | Gen: ethay airway onditioningcay isay orkingway\n","Epoch:  73 | Train loss: 0.009 | Val loss: 0.085 | Gen: ethay airway onditioningcay isay orkingway\n","Epoch:  74 | Train loss: 0.007 | Val loss: 0.082 | Gen: ethay airway onditioningcay isay orkingway\n","Epoch:  75 | Train loss: 0.006 | Val loss: 0.077 | Gen: ethay airway onditioningcay isay orkingway\n","Epoch:  76 | Train loss: 0.005 | Val loss: 0.077 | Gen: ethay airway onditioningcay isay orkingway\n","Epoch:  77 | Train loss: 0.005 | Val loss: 0.074 | Gen: ethay airway onditioningcay isay orkingway\n","Epoch:  78 | Train loss: 0.004 | Val loss: 0.072 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  79 | Train loss: 0.004 | Val loss: 0.071 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  80 | Train loss: 0.003 | Val loss: 0.071 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  81 | Train loss: 0.003 | Val loss: 0.070 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  82 | Train loss: 0.003 | Val loss: 0.069 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  83 | Train loss: 0.003 | Val loss: 0.068 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  84 | Train loss: 0.003 | Val loss: 0.069 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  85 | Train loss: 0.002 | Val loss: 0.069 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  86 | Train loss: 0.002 | Val loss: 0.071 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  87 | Train loss: 0.002 | Val loss: 0.072 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  88 | Train loss: 0.002 | Val loss: 0.071 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  89 | Train loss: 0.002 | Val loss: 0.070 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  90 | Train loss: 0.002 | Val loss: 0.064 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  91 | Train loss: 0.099 | Val loss: 0.292 | Gen: ehay airway onditioningcay isway orkingway\n","Epoch:  92 | Train loss: 0.128 | Val loss: 0.130 | Gen: edhay airway onditioningcay isway orkingway\n","Epoch:  93 | Train loss: 0.061 | Val loss: 0.128 | Gen: edhay airway onditioningcay isay orkingway\n","Epoch:  94 | Train loss: 0.048 | Val loss: 0.104 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  95 | Train loss: 0.013 | Val loss: 0.092 | Gen: edhay airway onditioningcay isay orkingway\n","Epoch:  96 | Train loss: 0.008 | Val loss: 0.085 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  97 | Train loss: 0.006 | Val loss: 0.082 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  98 | Train loss: 0.005 | Val loss: 0.081 | Gen: ehthay airway onditioningcay isway orkingway\n","Epoch:  99 | Train loss: 0.004 | Val loss: 0.077 | Gen: ehthay airway onditioningcay isway orkingway\n","source:\t\tthe air conditioning is working \n","translated:\tehthay airway onditioningcay isway orkingway\n"],"name":"stdout"}]},{"metadata":{"id":"vE-hKCxhF3iR","colab_type":"code","outputId":"1b371bc2-5092-4799-d3ab-ef992ef386ed","executionInfo":{"status":"ok","timestamp":1555975398819,"user_tz":240,"elapsed":448,"user":{"displayName":"Taehoon Jun","photoUrl":"","userId":"07855264162456456560"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["TEST_SENTENCE = 'i went shopping'\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["source:\t\ti went shopping \n","translated:\tiway entway oppingshay\n"],"name":"stdout"}]},{"metadata":{"id":"F8wbV63L5Kpj","colab_type":"code","outputId":"9dbbf335-f081-49ac-cd99-66cb5949d21b","executionInfo":{"status":"ok","timestamp":1555986367831,"user_tz":240,"elapsed":512003,"user":{"displayName":"Taehoon Jun","photoUrl":"","userId":"07855264162456456560"}},"colab":{"base_uri":"https://localhost:8080/","height":2526}},"cell_type":"code","source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n","              'attention_type': 'scaled_dot',  # options: additive / scaled_dot\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","rnn_attn_encoder, rnn_attn_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                                   cuda: 1                                      \n","                                nepochs: 100                                    \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.005                                  \n","                               lr_decay: 0.99                                   \n","                             batch_size: 64                                     \n","                            hidden_size: 20                                     \n","                           decoder_type: rnn_attention                          \n","                         attention_type: scaled_dot                             \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('advice', 'adviceway')\n","('soon', 'oonsay')\n","('excepting', 'exceptingway')\n","('loving', 'ovinglay')\n","('household', 'ouseholdhay')\n","Num unique word pairs: 6387\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNNAttentionDecoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MyGRUCell. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type ScaledDotAttention. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:   0 | Train loss: 2.279 | Val loss: 2.018 | Gen: ay-ay-ay-ay-ay away ongseday inay ongay\n","Epoch:   1 | Train loss: 1.816 | Val loss: 1.827 | Gen: elay ay onghay-ongay insay ontay\n","Epoch:   2 | Train loss: 1.627 | Val loss: 1.757 | Gen: elay away onsay-onsessay isay onday-onday\n","Epoch:   3 | Train loss: 1.490 | Val loss: 1.651 | Gen: eday ay ontionsensay-ongay-o issay oray-ongay\n","Epoch:   4 | Train loss: 1.382 | Val loss: 1.578 | Gen: etay away onghengway issay oray-ongay\n","Epoch:   5 | Train loss: 1.279 | Val loss: 1.515 | Gen: etay away onsinghengway isssay oodingway\n","Epoch:   6 | Train loss: 1.171 | Val loss: 1.402 | Gen: etay alay ondinghay isay oringhay\n","Epoch:   7 | Train loss: 1.070 | Val loss: 1.425 | Gen: etay alway ondinginghay-onghedw issay orkingigthay-ingway\n","Epoch:   8 | Train loss: 1.015 | Val loss: 1.256 | Gen: eatay ay-ayday ondingingway insay orkinghay\n","Epoch:   9 | Train loss: 0.875 | Val loss: 1.378 | Gen: etay alway ondinghay isway orighay\n","Epoch:  10 | Train loss: 0.814 | Val loss: 1.092 | Gen: etay ay-irway ondetingnay isay orkay-ingnay\n","Epoch:  11 | Train loss: 0.733 | Val loss: 1.213 | Gen: etay airay onditingway issay orkincay\n","Epoch:  12 | Train loss: 0.717 | Val loss: 0.984 | Gen: eay ay-ayday ondingingnay isway orkingnay\n","Epoch:  13 | Train loss: 0.696 | Val loss: 1.008 | Gen: etay airway ondingingnay isway orkingnay\n","Epoch:  14 | Train loss: 0.613 | Val loss: 1.206 | Gen: etay ay-ybay onditingway isway orkingway\n","Epoch:  15 | Train loss: 0.551 | Val loss: 0.818 | Gen: eay ay-awlay onditiningway isway orkingway\n","Epoch:  16 | Train loss: 0.494 | Val loss: 0.758 | Gen: eway airway onditiningenay isway orkingcay\n","Epoch:  17 | Train loss: 0.495 | Val loss: 0.678 | Gen: eway airway onditingnay isway orkingway\n","Epoch:  18 | Train loss: 0.451 | Val loss: 0.823 | Gen: etay ay-ayday ondititingcay isway orkingnay\n","Epoch:  19 | Train loss: 0.412 | Val loss: 0.650 | Gen: eway airway onditinningway isay orkingsay\n","Epoch:  20 | Train loss: 0.394 | Val loss: 0.647 | Gen: eay airway onditininingsibngay isway orkingngibay\n","Epoch:  21 | Train loss: 0.439 | Val loss: 0.971 | Gen: etay alwlay ondtttiboningway istay orkingcay\n","Epoch:  22 | Train loss: 0.461 | Val loss: 1.007 | Gen: etay aiwlay onditiongnay isay orkingstay\n","Epoch:  23 | Train loss: 0.489 | Val loss: 0.841 | Gen: eay aiblay onditiongspay isway orkingsay\n","Epoch:  24 | Train loss: 0.392 | Val loss: 0.710 | Gen: eway airway ondtitningway isway orkingway\n","Epoch:  25 | Train loss: 0.349 | Val loss: 0.548 | Gen: eway airway onditioningway isway orkingsay\n","Epoch:  26 | Train loss: 0.326 | Val loss: 0.467 | Gen: eway airway onditiongsay isway orkingway\n","Epoch:  27 | Train loss: 0.288 | Val loss: 0.453 | Gen: etay airway onditioningway isway orkingsay\n","Epoch:  28 | Train loss: 0.273 | Val loss: 0.418 | Gen: etay airway onditiongscay isway orkingday\n","Epoch:  29 | Train loss: 0.248 | Val loss: 0.539 | Gen: etay airway onditioningway isway orkingday\n","Epoch:  30 | Train loss: 0.276 | Val loss: 0.862 | Gen: etay airway onditiongscay isway orkingday\n","Epoch:  31 | Train loss: 0.350 | Val loss: 0.824 | Gen: etay airway onditingsay isway orkingway\n","Epoch:  32 | Train loss: 0.308 | Val loss: 0.520 | Gen: etay airway onditioningway isway orkingway\n","Epoch:  33 | Train loss: 0.268 | Val loss: 0.443 | Gen: etay airway onditioningcay isway orkingday\n","Epoch:  34 | Train loss: 0.219 | Val loss: 0.402 | Gen: etay airway onditioningsay isway orkingday\n","Epoch:  35 | Train loss: 0.203 | Val loss: 0.359 | Gen: etay airway onditioningsay isway orkingday\n","Epoch:  36 | Train loss: 0.216 | Val loss: 1.362 | Gen: etay airsay ondititioioningningn isway orkingvay\n","Epoch:  37 | Train loss: 0.309 | Val loss: 0.980 | Gen: etay airsay ondititiningway isway orkingway\n","Epoch:  38 | Train loss: 0.284 | Val loss: 0.693 | Gen: etay away-ayday ondititingcay isway orkingsay\n","Epoch:  39 | Train loss: 0.323 | Val loss: 0.817 | Gen: etay airway onditiongnay isway orkingsay\n","Epoch:  40 | Train loss: 0.263 | Val loss: 1.111 | Gen: etay airway onditingsay isway orkingway\n","Epoch:  41 | Train loss: 0.274 | Val loss: 0.539 | Gen: etay airsay onditioningsway isway orkingsay\n","Epoch:  42 | Train loss: 0.218 | Val loss: 0.356 | Gen: etay airway onditioningcay isway orkingsay\n","Epoch:  43 | Train loss: 0.175 | Val loss: 0.325 | Gen: etay airway onditioningsay isway orkingsay\n","Epoch:  44 | Train loss: 0.158 | Val loss: 0.320 | Gen: etay airway onditioningcay isway orkingsay\n","Epoch:  45 | Train loss: 0.148 | Val loss: 0.326 | Gen: etay airway onditioningcay isway orkingsay\n","Epoch:  46 | Train loss: 0.136 | Val loss: 0.331 | Gen: etay airway onditioningsay isway orkingsay\n","Epoch:  47 | Train loss: 0.130 | Val loss: 0.325 | Gen: etay airway onditioningsay isway orkingsay\n","Epoch:  48 | Train loss: 0.122 | Val loss: 0.318 | Gen: etay airway onditioningsay isway orkingway\n","Epoch:  49 | Train loss: 0.114 | Val loss: 0.335 | Gen: etay airway onditioningsay isway orkingway\n","Epoch:  50 | Train loss: 0.113 | Val loss: 0.605 | Gen: ethay airway onditioningsay isway orkingway\n","Epoch:  51 | Train loss: 0.377 | Val loss: 0.651 | Gen: etay awaray onditioningsay isay orkingway\n","Epoch:  52 | Train loss: 0.230 | Val loss: 0.543 | Gen: etay airway onditioningsay isway orkingnay\n","Epoch:  53 | Train loss: 0.184 | Val loss: 0.416 | Gen: ethay airway onditioningsay isway orkingway\n","Epoch:  54 | Train loss: 0.131 | Val loss: 0.298 | Gen: ethay airway onditioningsay isway orkingway\n","Epoch:  55 | Train loss: 0.116 | Val loss: 0.279 | Gen: ethay airway onditioningsay isway orkingway\n","Epoch:  56 | Train loss: 0.106 | Val loss: 0.266 | Gen: ethay airway onditioningsay isway orkingway\n","Epoch:  57 | Train loss: 0.096 | Val loss: 0.277 | Gen: ethay airway onditioningjay isway orkingway\n","Epoch:  58 | Train loss: 0.088 | Val loss: 0.268 | Gen: ethay airway onditioningsay isway orkingway\n","Epoch:  59 | Train loss: 0.082 | Val loss: 0.272 | Gen: ethay airway onditioningjay isway orkingway\n","Epoch:  60 | Train loss: 0.077 | Val loss: 0.266 | Gen: ethay airway onditioningjay isway orkingway\n","Epoch:  61 | Train loss: 0.073 | Val loss: 0.276 | Gen: ethay airway onditioningjay isway orkingway\n","Epoch:  62 | Train loss: 0.071 | Val loss: 0.282 | Gen: ethay airway onditioningjay isway orkingway\n","Epoch:  63 | Train loss: 0.157 | Val loss: 0.994 | Gen: etay airway onditiongsay isway orkingway\n","Epoch:  64 | Train loss: 0.293 | Val loss: 1.118 | Gen: egay airsay onditioungsay istay orgingstay\n","Epoch:  65 | Train loss: 0.332 | Val loss: 0.825 | Gen: etay airway onditioningcay isway orkingcay\n","Epoch:  66 | Train loss: 0.198 | Val loss: 0.445 | Gen: etay airway onditioningcay isway orkingway\n","Epoch:  67 | Train loss: 0.126 | Val loss: 0.273 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  68 | Train loss: 0.095 | Val loss: 0.307 | Gen: etay airway onditioningsay isway orkingway\n","Epoch:  69 | Train loss: 0.080 | Val loss: 0.290 | Gen: etay airway onditioningsay isway orkingway\n","Epoch:  70 | Train loss: 0.071 | Val loss: 0.261 | Gen: etay airway onditioningsay isway orkingway\n","Epoch:  71 | Train loss: 0.065 | Val loss: 0.238 | Gen: etay airway onditioningcay isway orkingway\n","Epoch:  72 | Train loss: 0.060 | Val loss: 0.229 | Gen: etay airway onditioningcay isway orkingway\n","Epoch:  73 | Train loss: 0.057 | Val loss: 0.221 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  74 | Train loss: 0.054 | Val loss: 0.217 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  75 | Train loss: 0.051 | Val loss: 0.213 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  76 | Train loss: 0.047 | Val loss: 0.204 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  77 | Train loss: 0.044 | Val loss: 0.195 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  78 | Train loss: 0.042 | Val loss: 0.207 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  79 | Train loss: 0.041 | Val loss: 0.201 | Gen: etay airway onditioningcay isway orkingway\n","Epoch:  80 | Train loss: 0.041 | Val loss: 0.209 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  81 | Train loss: 0.043 | Val loss: 0.219 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  82 | Train loss: 0.076 | Val loss: 0.872 | Gen: ethay airway onditioningway isway orkingway\n","Epoch:  83 | Train loss: 0.286 | Val loss: 0.734 | Gen: etay airway ondititingcay isway orkingway\n","Epoch:  84 | Train loss: 0.198 | Val loss: 0.349 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  85 | Train loss: 0.122 | Val loss: 0.745 | Gen: ethay airway ondititioningcay isway orkingway\n","Epoch:  86 | Train loss: 0.125 | Val loss: 0.334 | Gen: etay airway onditioningcay isway orkingway\n","Epoch:  87 | Train loss: 0.076 | Val loss: 0.238 | Gen: etay airway onditioningway isway orkingway\n","Epoch:  88 | Train loss: 0.056 | Val loss: 0.233 | Gen: etay airway onditioningcay isway orkingway\n","Epoch:  89 | Train loss: 0.047 | Val loss: 0.235 | Gen: etay airway onditioningcay isway orkingway\n","Epoch:  90 | Train loss: 0.044 | Val loss: 0.193 | Gen: etay airway onditioningcay isway orkingway\n","Epoch:  91 | Train loss: 0.042 | Val loss: 0.183 | Gen: etay airway onditioningcay isway orkingway\n","Epoch:  92 | Train loss: 0.041 | Val loss: 0.168 | Gen: etay airway onditioningcay isway orkingclay\n","Epoch:  93 | Train loss: 0.036 | Val loss: 0.161 | Gen: etay airway onditioningcay isway orkingway\n","Epoch:  94 | Train loss: 0.033 | Val loss: 0.166 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  95 | Train loss: 0.031 | Val loss: 0.164 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  96 | Train loss: 0.029 | Val loss: 0.162 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  97 | Train loss: 0.028 | Val loss: 0.155 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  98 | Train loss: 0.026 | Val loss: 0.157 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  99 | Train loss: 0.025 | Val loss: 0.150 | Gen: ethay airway onditioningcay isway orkingway\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway onditioningcay isway orkingway\n"],"name":"stdout"}]},{"metadata":{"id":"aDsBc3kd5NKO","colab_type":"code","outputId":"d0236abf-a316-4122-ccc4-3d4dde170b18","executionInfo":{"status":"ok","timestamp":1555986385769,"user_tz":240,"elapsed":364,"user":{"displayName":"Taehoon Jun","photoUrl":"","userId":"07855264162456456560"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"cell_type":"code","source":["TEST_SENTENCE = 'i went shopping'\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["source:\t\ti went shopping \n","translated:\tiway entway oppingshay\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"metadata":{"id":"X8FaZZUWRpY9","colab_type":"text"},"cell_type":"markdown","source":["## Transformer"]},{"metadata":{"id":"Ik5rx9qw9KCg","colab_type":"code","outputId":"8b93ec6e-f937-4e93-a0f5-c7b8cec22b6e","executionInfo":{"status":"error","timestamp":1555998400256,"user_tz":240,"elapsed":424,"user":{"displayName":"Taehoon Jun","photoUrl":"","userId":"07855264162456456560"}},"colab":{"base_uri":"https://localhost:8080/","height":1744}},"cell_type":"code","source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n","              'num_transformer_layers': 3,\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","transformer_encoder, transformer_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":69,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                                   cuda: 1                                      \n","                                nepochs: 100                                    \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.005                                  \n","                               lr_decay: 0.99                                   \n","                             batch_size: 64                                     \n","                            hidden_size: 20                                     \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 3                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('instantaneous', 'instantaneousway')\n","('tediously', 'ediouslytay')\n","('avarice', 'avariceway')\n","('assemblies', 'assembliesway')\n","('further', 'urtherfay')\n","Num unique word pairs: 6387\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-111543f220b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-de978f00c017>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opts)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exiting early from training.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-de978f00c017>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-de978f00c017>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mdecoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gets decoder inputs by shifting the targets to the right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_annotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mdecoder_outputs_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mtargets_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-68-927351767e0c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, annotations, hidden_init)\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mresidual_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m           \u001b[0mnew_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_attentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m           \u001b[0mresidual_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0mnew_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mlps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-42ca017e082e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, queries, keys, values)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m           \u001b[0mqur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [64 x 40], m2: [20 x 20] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:266"]}]},{"metadata":{"id":"ULCMHm5ZF7vx","colab_type":"code","colab":{}},"cell_type":"code","source":["TEST_SENTENCE = 'the air conditioning is working'\n","translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qbfZCByITOI6","colab_type":"text"},"cell_type":"markdown","source":["# Attention visualization"]},{"metadata":{"id":"itCGMv3FdXsn","colab_type":"code","colab":{}},"cell_type":"code","source":["TEST_WORD_ATTN = 'aardvark'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xBv4QQuBiU-V","colab_type":"text"},"cell_type":"markdown","source":["## Visualize RNN attention map"]},{"metadata":{"id":"aXvqoQYONMTA","colab_type":"code","outputId":"a27e5bb9-0d68-4049-fbcd-ca73ebc65965","executionInfo":{"status":"ok","timestamp":1553482985190,"user_tz":240,"elapsed":1264,"user":{"displayName":"Taehoon Jun","photoUrl":"","userId":"07855264162456456560"}},"colab":{"base_uri":"https://localhost:8080/","height":418}},"cell_type":"code","source":["visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAgsAAAGACAYAAAAjwCFIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n","text/plain":["<Figure size 576x396 with 2 Axes>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["'ablvarsay'"]},"metadata":{"tags":[]},"execution_count":77}]},{"metadata":{"id":"xuOvxfA1NMz3","colab_type":"text"},"cell_type":"markdown","source":["## Visualize transformer attention maps from all the transformer layers"]},{"metadata":{"id":"HSSB4wd8-M7g","colab_type":"code","outputId":"9eb0df7e-bcb1-4ca7-d527-9a783de78762","executionInfo":{"status":"error","timestamp":1553482147682,"user_tz":240,"elapsed":615,"user":{"displayName":"Taehoon Jun","photoUrl":"","userId":"07855264162456456560"}},"colab":{"base_uri":"https://localhost:8080/","height":182}},"cell_type":"code","source":["visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-64-83988dad20a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_WORD_ATTN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'transformer_encoder' is not defined"]}]}]}